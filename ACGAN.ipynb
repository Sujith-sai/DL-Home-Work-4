{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14113341-e6a9-4c0e-9939-f4fae940b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cee0dcc-6018-4a7d-95a6-7650933f566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = datasets.CIFAR10(root=\"./data\", download=False, transform=transforms.Compose(\n",
    "    [transforms.Resize(64),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size = 128, shuffle=True, num_workers=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb83690-40e0-4302-858f-1647685ce51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee579afa-76ef-4eb7-a905-20c2ccf52c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1,bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(64,128,4,2,1,bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(128,256,4,2,1,bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(256,512,4,2,1,bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,True)\n",
    "        )\n",
    "        \n",
    "        self.verify = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.labels = nn.Sequential(\n",
    "            nn.Conv2d(512, 11, 4, 1, 0, bias = False), \n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, passed_input):\n",
    "        passed_input = self.main(passed_input)\n",
    "        validity = self.verify(passed_input)\n",
    "        output_labels = self.labels(passed_input)\n",
    "        \n",
    "        # resize\n",
    "        validity = validity.view(-1)\n",
    "        output_labels = output_labels.view(-1,11)\n",
    "        return validity, output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec1c92-368b-4e48-8660-c9fafd69f22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcffde37-9706-46f7-8005-c966def75ea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lg/_nxwhdjj5g50f5b5qlzp5lxr0000gn/T/ipykernel_36654/3703708093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "generator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a7684-4342-4429-87eb-724f7a3c0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_optim = optim.Adam(discriminator.parameters(), 0.0002, betas = (0.5,0.999))\n",
    "gen_optim = optim.Adam(generator.parameters(), 0.0002, betas = (0.5,0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "real_labels_tensor = 0.7 + 0.5 * torch.rand(10, device = device)\n",
    "fake_labels_tensor = 0.3 * torch.rand(10, device = device)\n",
    "\n",
    "\n",
    "counter_list = []\n",
    "counter = 0\n",
    "gen_loss_list = []\n",
    "dis_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e50ab-dc5a-41a4-ba68-a56d11823e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    # Iterate through all batches\n",
    "    for index, (data, image_labels) in enumerate(data_loader, 0):\n",
    "        counter += 1\n",
    "        counter_list.append(counter)\n",
    "        \n",
    "        # make data avaialbe for cuda\n",
    "        data = data.to(device)\n",
    "        image_labels = image_labels.to(device)\n",
    "        size_of_batch = data.size(0)\n",
    "        labels_real = real_labels_tensor[index % 10]\n",
    "        labels_fake = fake_labels_tensor[index % 10]\n",
    "        class_labels_fake = 10 * torch.ones((size_of_batch, ), dtype = torch.long, device = device)\n",
    "        \n",
    "        # Periodically switch labels\n",
    "        if index % 25 == 0:\n",
    "            temp = labels_real\n",
    "            labels_real = labels_fake\n",
    "            labels_fake = temp\n",
    "        \n",
    "        # Train Discriminator with real data\n",
    "        labels_for_validate = torch.full((size_of_batch, ), labels_real, device = device)\n",
    "        dis_optim.zero_grad() \n",
    "        validity, output_labels = discriminator(data)       \n",
    "        dis_real_valid_error = criterion(validity, labels_for_validate)            \n",
    "        dis_real_label_error = F.nll_loss(output_labels, image_labels)\n",
    "        dis_real_error = dis_real_valid_error + dis_real_label_error\n",
    "        dis_real_error.backward()\n",
    "        valid_mean1 = validity.mean().item()        \n",
    "        \n",
    "        # Train Discriminator with fake data\n",
    "        dis_fake_labels = torch.randint(0, 10, (size_of_batch, ), dtype = torch.long, device = device)\n",
    "        noise = torch.randn(size_of_batch, 100, device = device)  \n",
    "        labels_for_validate.fill_(labels_fake)\n",
    "        fake_output = generator(noise, dis_fake_labels)\n",
    "        validity, output_labels = discriminator(fake_output.detach())       \n",
    "        dis_fake_valid_error = criterion(validity, labels_for_validate)\n",
    "        dis_fake_label_error = F.nll_loss(output_labels, class_labels_fake)\n",
    "        dis_fake_error = dis_fake_valid_error + dis_fake_label_error\n",
    "        dis_fake_error.backward()\n",
    "        final_dis_error = dis_real_error + dis_fake_error\n",
    "        valid_mean2 = validity.mean().item()\n",
    "        dis_optim.step()\n",
    "    \n",
    "        # Train Generator\n",
    "        labels_for_validate.fill_(1)\n",
    "        labels_for_gen = torch.randint(0, 10, (size_of_batch, ), device = device, dtype = torch.long)\n",
    "        noise = torch.randn(size_of_batch, 100, device = device)  \n",
    "        gen_optim .zero_grad()\n",
    "        fake_output = generator(noise, labels_for_gen)\n",
    "        validity, output_labels = discriminator(fake_output)\n",
    "        gen_valid_error = criterion(validity, labels_for_validate)        \n",
    "        gen_label_error = F.nll_loss(output_labels, labels_for_gen)\n",
    "        final_gen_error = gen_valid_error + gen_label_error\n",
    "        final_gen_error.backward()\n",
    "        valid_mean3 = validity.mean().item()\n",
    "        gen_optim .step()\n",
    "        \n",
    "        \n",
    "        print(\"[{}/{}] [{}/{}] D(x): [{:.4f}] D(G): [{:.4f}/{:.4f}] GLoss: [{:.4f}] DLoss: [{:.4f}] DLabel: [{:.4f}] \"\n",
    "              .format(epoch, num_epochs, index, len(data_loader), valid_mean1, valid_mean2, valid_mean3, final_gen_error, final_dis_error,\n",
    "                      dis_real_label_error+ dis_fake_label_error + gen_label_error))\n",
    "        \n",
    "        # Save errors for graphing\n",
    "        dis_loss_list.append(final_dis_error.cpu().detach().numpy())\n",
    "        gen_loss_list.append(final_gen_error.cpu().detach().numpy())\n",
    "        \n",
    "    # Save images to folder\n",
    "    labels = torch.arange(0,10,dtype = torch.long,device = device)\n",
    "    noise = torch.randn(10,100,device = device)  \n",
    "    images = generator(noise, labels)\n",
    "    vutils.save_image(images.detach(),'ACGANOutput/fake_samples_epoch_%03d.png' % (epoch), normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e15a3f-53d0-4d15-b94f-cca3c54999fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(counter_list, gen_loss_list, 'r.', label='Generator')\n",
    "plt.plot(counter_list, dis_loss_list, 'g.', label='Discriminator')\n",
    "plt.title(\"ACGAN Loss of Discriminator and Generator\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss (Binary Cross Entropy)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
